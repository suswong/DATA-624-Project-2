---
title: "DATA 624 Project"
author: "Susanna Wong, Ana Collado, Gullit Navarrete"
date: "2025-12-01"
output:
  html_document:
    theme: cerulean
    toc: true
    toc_float: true
  word_document:
    toc: true
  pdf_document:
    toc: true
editor_options:
  markdown:
    wrap: sentence
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(DataExplorer)
library(dplyr)
library(psych)
library(ggplot2)
library(tidyverse)
library(janitor)
library(interactions)
library(kableExtra)
library(pracma)
library(randomForest)
library(caret)
library(partykit)
library(car)

```
# Objective

# Introduction

The goal of the project is to understand the manufacturing process and the predictive factor of the beverage. Create a predictive model of PH.



# Data Exploration

## Load Data

```{r}
library(dplyr)
data <- read.csv("https://raw.githubusercontent.com/suswong/DATA-624/refs/heads/main/StudentData.csv", na.strings = c("", "NA"))

s.eval <- read.csv("https://raw.githubusercontent.com/suswong/DATA-624/refs/heads/main/StudentEvaluation.csv", na.strings = c("", "NA"))
```

```{r}
head(data)
```

There are 2571 observations with 33 variables. 

```{r}
summary(data)
```
## Renaming Variables

Column names were cleaned and standardized to make them more consistent and easier to work with in R. 
- Spaces and special character were removed. 
- Renamed `Carb.Pressure` and `Carb.Pressure1` to `CarbPressure1` and `CarbPressure2` to following the naming of `HydPressure1` - `HydPressure4`

```{r}
names(data) <- c("BrandCode", "CarbVolume","FillOunces", "PCVolume","CarbPressure1","CarbTemp", "PSC", "PSCFill", "PSCCO2", "MnfFlow", "CarbPressure2","FillPressure", "HydPressure1", "HydPressure2",     "HydPressure3",     "HydPressure4",    
"FillerLevel",      "FillerSpeed" ,     "Temperature",       "Usagecont",       
"CarbFlow" ,        "Density"   ,        "MFR"       ,        "Balling" ,         
"PressureVacuum" ,  "PH"    ,            "OxygenFiller" ,    "BowlSetpoint"   , 
"PressureSetpoint", "AirPressurer",    "AlchRel" ,         "CarbRel"  ,       
"BallingLvl" )

names(s.eval) <- c("BrandCode", "CarbVolume","FillOunces", "PCVolume","CarbPressure1","CarbTemp", "PSC", "PSCFill", "PSC CO2", "MnfFlow", "CarbPressure2","FillPressure", "HydPressure1", "HydPressure2",     "HydPressure3",     "HydPressure4",    
"FillerLevel",      "FillerSpeed" ,     "Temperature",       "Usagecont",       
"CarbFlow" ,        "Density"   ,        "MFR"       ,        "Balling" ,         
"PressureVacuum" ,  "PH"    ,            "OxygenFiller" ,    "BowlSetpoint"   , 
"PressureSetpoint", "AirPressurer",    "AlchRel" ,         "CarbRel"  ,       
"BallingLvl" )
```

## Missing Data

```{r}
library(DataExplorer)
plot_intro(data)
```

844 observations contains missing data. 

```{r}
na_summary<- data.frame(variable= names(data), na_count= colSums(is.na(data)),
                        percent_na= colMeans(is.na(data)) * 100) %>%
  arrange(desc(percent_na))
na_summary
na_summary %>%
  summarize(total_na= sum(na_count))
```

Every column has missing values except `Pressure.Vacuum` and `Air.Pressurer`.

```{r}
sapply(data, function(x) sum(is.na(x)))
```

Below is the percentage of missing data for each column. `MFR` has the highest percentage of missing data but it is only accounts about 8.25%. The amount of missing values is not significant so we should not drop any observations with missing values. We should impute the missing values later. 

```{r, message=FALSE}
library(DataExplorer)
plot_missing(data,
             missing_only = T)
```

```{r}
data$BrandCode <- as.factor(data$BrandCode)
```

## Distribution

Variables with good range and clear distribution is likely to show a relationship with the dependent variable. The following variables appears normal distribution:
- Carb Pressure 1
- Carb Pressure 2
- Carb Temp
- Carb Volume
- Fill Ounces
- PC Volume

Variables that appears skewed will need transformation to be performed. 
The following variables appears right skewed:
- Hyd Pressure 1 (There is a significant amount of values at 0) 
- PSC                
- PSC Fill          
- PSC CO2
- Pressure Vacuum    
- Temperature
- Oxygen Filler 

The following variables appears left skewed:  
- Hyd Pressure2 (There is a significant amount of values at 0)     
- Hyd Pressure3 (There is a significant amount of values at 0)      
- Hyd Pressure4  
- Mnf Flow  (There is a significant amount of values at 0)          
- MFR
         
The following variables appear bimodal. 
- Balling 
- Balling Lvl  
- Carb Rel 
- Density

Variables that are highly multimodal can reveal that they may be operated at different setting or conditions. Later we may want to transform these discrete variables into factors so that the model treats them as distinct operational settings.The following variables appear multimodal. 
- Bowl Set Point
- Pressure Set Point


```{r}
plot_histogram(data)
```

Almost half of the observations are brand C. Brands A and C have similar observations.

```{r}
library(ggplot2)
ggplot(data, 
       aes(x = BrandCode, fill = BrandCode)) +
  geom_bar() +
  theme_minimal() + 
  theme(legend.position = "none") +
  labs( title = "Brand Frequency",
        x = "Brand",
        y = "Count")
```

Brand C has a lower PH while Brand D has the highest PH level.

```{r}
ggplot(data, 
       aes(x = BrandCode, y = PH, fill = BrandCode)) +
  geom_boxplot() +
  theme_minimal() + 
  theme(legend.position = "none") +
  labs( title = "Boxplot of PH by Brand",
        x = "Brand",
        y = "PH")
```

## Transforming Variables to factors

Variables that are highly multimodal can reveal that they may be operated at different setting or conditions. Later we may want to transform these discrete variables into factors so that the model treats them as distinct operational settings.

```{r}
train_data <- data %>%
  mutate(
    PressureSetpoint = as.factor(PressureSetpoint),
    BowlSetpoint = as.factor(BowlSetpoint),
    BrandCode = as.factor(BrandCode)
  )

s.eval <- s.eval %>%
  mutate(
    PressureSetpoint = as.factor(PressureSetpoint),
    BowlSetpoint = as.factor(BowlSetpoint),
    BrandCode = as.factor(BrandCode)
  )
```

# Preprocessing

`PH`, the target variable has missing values. We should drop any observations that contains missing values in `PH`

`HydPressure1` was dropped as it has near zero variance. 

```{r}
library(tidyr)
library(caret)
train_clean_data <- data %>%
  drop_na(PH)

train_filtered <- train_clean_data[, -nearZeroVar(train_clean_data)]

```

## Imputing Missing Data

```{r}
set.seed(1)
trainingRows <- createDataPartition(train_filtered$PH,
                                    p = .70,
                                    list= FALSE)
train1 <- train_filtered[trainingRows, ]
  
test1  <- train_filtered[-trainingRows, ]
```

```{r}
library(forcats)

train1 <- train1%>%
  mutate(across(where(is.factor), ~fct_na_value_to_level(.x, "Unknown")))

test1 <- test1%>%
  mutate(across(where(is.factor), ~fct_na_value_to_level(.x, "Unknown")))
```

```{r}
trans <- preProcess(train1 %>%
                      select(-PH), 
                    method = c("center", "scale", "knnImpute", "BoxCox"))

transformed_data <- predict(trans, train1%>%
                     select(-PH))
train_final <- cbind(transformed_data, PH = train1$PH)

test_transformed <- predict(trans, test1%>%
                     select(-PH))

test_final <- cbind(test_transformed, PH = test1$PH)
```

# Correlation

The correlation plot reveals multicollienarity is present. 

```{r}
library(corrplot)
corrplot(cor(train_final %>%
               select(where(is.numeric)),
             use = "pairwise.complete.obs"),
         type = "lower",
         order = "alphabet",
         tl.cex = 0.6,
         tl.col = "black")
```

# Modeling

## Simple Linear Regression

This model is simple, interpret able, and creates a good baseline. 

### Model 1

```{r}
set.seed(1)

lm_model <- train(PH ~. , 
                  data = train_final,
                  method = "lm", 
                  trControl = trainControl(method = "cv", number = 5)
                  )

summary(lm_model)
```

```{r}
plot(lm_model$finalModel)
```

```{r}
lm_pred <- predict(lm_model, test_final)
postResample(pred = lm_pred, obs = test_final$PH)
```

Earlier in the correlation plot, we saw multicollinearity is present. The Variance Inflation Factor can help identify variables that causes multicollinearity. We should drop high-VIF predictors. 

```{r, message=False}
library(car)
vif(lm_model$finalModel)
```

BallingLvl has the highest VIF value. Dropping it for our next models. 

### Model 2

```{r}
train_final_reduced <- train_final %>%
  select(-BallingLvl)

set.seed(2)
lm_model2 <- train(PH ~. , 
                  data = train_final_reduced,
                  method = "lm", 
                  trControl = trainControl(method = "cv", number = 5)
                  )

summary(lm_model2)
```
```{r}
lm_pred2 <- predict(lm_model2, train_final_reduced)
postResample(pred = lm_pred2, obs = train_final_reduced$PH)
```

```{r}
vif(lm_model2$finalModel)
```

### Model 3

```{r}
train_final_reduced <- train_final %>%
  select(-BallingLvl, -Balling)

set.seed(2)
lm_model3 <- train(PH ~. , 
                  data = train_final_reduced,
                  method = "lm", 
                  trControl = trainControl(method = "cv", number = 5)
                  )

summary(lm_model3)
```

```{r}
lm_pred3 <- predict(lm_model3, train_final_reduced)
postResample(pred = lm_pred3, obs = train_final_reduced$PH)
```

## Ridge

Ridge can handle high correlated predictors by shrinking their coefficients.

```{r}
set.seed(11)

ridge_model <- train(PH ~. , 
                  data = train_final,
                  method = "ridge", 
                  trControl = trainControl(method = "cv", number = 5)
                  )

summary(ridge_model)
```

```{r}
ridge_pred <- predict(ridge_model, test_final)
postResample(pred = ridge_pred, obs = test_final$PH)
```

## elastic net

```{r}
set.seed(111)

enet_model <- train(PH ~. , 
                  data = train_final,
                  method = "enet", 
                  trControl = trainControl(method = "cv", number = 5)
                  )

summary(enet_model)
```

```{r}
enet_pred <- predict(enet_model, test_final)
postResample(pred = enet_pred, obs = test_final$PH)
```


## Random Forest

```{r}
set.seed(11111)

rf_model <- train(PH ~. , 
                  data = train_final,
                  method = "rf", 
                  trControl = trainControl(method = "cv", number = 5),
                  tuneLength = 5
                  )

summary(rf_model)
```

```{r}
rf_pred <- predict(rf_model, test_final)
postResample(pred = rf_pred, obs = test_final$PH)
```

## SVM

## Model 1 

There are several types of SVM. We will use radial basis kernel.

```{r}
set.seed(11111)

svm_model <- train(PH ~. , 
                  data = train_final,
                  method = "svmRadial", 
                  trControl = trainControl(method = "cv", number = 5),
                  tuneLength = 10
                  )

summary(svm_model)
```

```{r}
svm_pred <- predict(svm_model, test_final)
postResample(pred = svm_pred, obs = test_final$PH)
```

## Model 2

```{r}
set.seed(11111)

svm_model2 <- train(PH ~. , 
                  data = train_final,
                  method = "svmPoly",
                  trControl = trainControl(method = "cv", number = 5),
                  tuneLength = 5
                  )

summary(svm_model2)
```

```{r}
svm_pred2 <- predict(svm_model2, test_final)
postResample(pred = svm_pred2, obs = test_final$PH)
```

## KNN

```{r}
set.seed(111111)

knn_model <- train(PH ~. , 
                  data = train_final,
                  method = "knn", 
                  preProc = c("scale", "center"),
                  trControl = trainControl(method = "cv", number = 5),
                  tuneLength = 10
                  )

plot(knn_model)
```

```{r}
knn_pred <- predict(knn_model, test_final)
postResample(pred = knn_pred, obs = test_final$PH)
```


```{r}
set.seed(111112)

knn_model2 <- train(PH ~. , 
                  data = train_final,
                  method = "kknn", 
                  preProc = c("scale", "center"),
                  trControl = trainControl(method = "cv", number = 5),
                  tuneGrid = expand.grid(kmax = seq(3,30,2), 
                                         distance = 2,
                                         kernel = c("rectangular", "triangular","epanechnikov")))
plot(knn_model2)
```

```{r}
knn_pred2 <- predict(knn_model2, test_final)
postResample(pred = knn_pred2, obs = test_final$PH)
```

## gbm or xgboost


## cubist

```{r}
set.seed(1111111)

cubistGrid <- expand.grid(committees = c(5, 10, 20, 25, 30, 35),
                          neighbors = c(3, 5, 7, 9))

cubist_model1 <- train(PH ~. , 
                  data = train_final,
                  method = "cubist",
                  trControl = trainControl(method = "cv", number = 5),
                  tuneGrid = cubistGrid)
cubist_model1
```

```{r}
cubist_pred <- predict(cubist_model1, test_final)
postResample(pred = cubist_pred, obs = test_final$PH)
```

------------------------------------------------------------------------

# Linear Model- Multicollinearity

Linear regression does not require the predictor variables to be normally distributed which works here considering many of the distributions of the variables are not normal.We've taken a look at a linear model but we'll address the multicolinearity of the datase.
In this section we'll restrict which variables will be fed into the model.
Milticollinearity can effect the output coefficients and the model interpretatibility.
We can't afford to lose grip on these in reporting to our stakeholders.
To assess this, we'll separate the response variable from the predictors.

```{r correlation_table_v2}
predictors <- data[, names(data) != "ph"]
response   <- data$ph
ph_corr <- corr.test(x = predictors[,-1], y = response, "pairwise", "pearson")
correlations <- ph_corr$r[, 1]
p_values <- ph_corr$p[, 1]
ph_table <- data.frame(r= correlations, p= p_values)
ph_table %>% 
  arrange(desc(r))
```

The `correlation table r` shows the strength and direction of the relationship of each variable with our response variable `ph`. The Pairwise calculations ignore NA values which is useful.
Next, we'll find correlations without the response variable to determine pairwise redundancies/ multicollinearity.
We'll use findCorrelation() from the caret package which will use the cutoff 0.7 (Lin, 99) as threshold to point out the clusters.

```{r predictor_v_predictor}
pred_numerical <- predictors[, sapply(predictors, is.numeric)]
ppcor<- pred_cor <- cor(pred_numerical, use = "pairwise.complete.obs")
findCorrelation(ppcor, cutoff = 0.7)
```

The function prints the column numbers.
We'll have it output the column names

```{r multicolinearity}
colnames(pred_numerical)[findCorrelation(ppcor, cutoff = 0.7)]
```

The named variables are flagged as highly correlated clusters, above our threshold.
We'll examine the correlation plot

```{r corr_plot, fig.align='center', fig.width= 8, fig.length= 8}
corrplot(ppcor, title ="Predictor v Predictor Correlation", outline = TRUE, type = "full")
```

The correlation plot shows variables related to liquid density including CarbVolume, Density, Balling, BallingLvl, CarbRel, and AlchRel have pairwise correlations above 0.7.
Though different measurements, these measure density and will be redundant in a linear regression model.
To preserve the model's interpretability, we'll run "variance inflation factor" vif() from the `car` package to double check.

Further, variables that share measurements of flow are also identified as above the 0.7 threshold.
HydPressure2, HydPressure3, FillerLevel, FillerSpeed.
Of these CarbTemp is

```{r}
#--Density variables
lm2a <- lm(PH ~ Density+ Balling+ BallingLvl+ CarbRel+ AlchRel,data = data)
vif(lm2a)
summary(lm2a)

#-- dropping redundant variables
lm2b <- lm(PH ~ Density + CarbRel, data = data)
vif(lm2b)

data2<- data %>% 
  select(-c(Balling, BallingLvl, AlchRel))

#--Flow variables
lm3a <- lm(PH ~ MnfFlow+ HydPressure3+ HydPressure2+ FillerLevel+ FillerSpeed,data = data)
vif(lm3a) 
summary(lm3a)

data2<- data2 %>% 
  select(-c(HydPressure3))

lm_full <- lm(PH ~ ., data = data2)
summary(lm_full)
plot(lm_full, which = 1)

plot(lm_full, which = 2)
shapiro.test(residuals(lm_full))
```

```{r include=TRUE, eval=FALSE}
qqnorm(residuals(lm_full))+
qqline(residuals(lm_full))
```

![](images/m.linear_qqplot)

```{r}
n<- nrow(data)
test_id<- sample(seq_len(n), size = floor(0.30 * n))

test_a  <- data[test_id, ]
train_a <- data[-test_id, ]

test_a$PH_hat <- predict(lm_full, newdata = test_a)
rmse <- sqrt(mean((test_a$PH- test_a$PH_hat)^2, na.rm = TRUE))
mae  <- mean(abs(test_a$PH- test_a$PH_hat), na.rm = TRUE)

c(RMSE = rmse, MAE = mae)
```

```{r multi-obs_v_preditcted}
ggplot(test_a, aes(x = PH, y = PH_hat)) +
  geom_point(alpha = 0.6, position = "jitter") +
  geom_abline(intercept = 0, linetype = "dashed", colour = "blue") +
  labs(
    title = "Observed vs Predicted PH",
    x = "Observed",
    y = "Predicted") +
  theme_minimal()
```

The model can explain \~43% of the variance in the data.
The final model outputs an RMSE of about 0.13 and MAE of 0.10.
While in most cases the difference of 0.1 points to a decent model, in the case of ph we should attempt to find a more accurate model.

# Random Forest

Random Forest doesn't not require handling of multicolinearity.

```{r data_split}
data <- data %>%
  clean_names() %>%
  na.omit()


n<- nrow(data)
test_id<- sample(seq_len(n), size = floor(0.30 * n))

test_a  <- data[test_id, ]
train_a <- data[-test_id, ]
```

```{r train}
library(randomForest)
library(caret)
library(partykit)
library(janitor)

lm4a <- randomForest(ph ~ .,data= train_a, importance = TRUE, ntree = 10000)
rfImp1 <- varImp(lm4a, scale = FALSE)
print(importance(lm4a))

varImpPlot(lm4a, sort = TRUE, main = "Variable Importance using VarImpPlot()")
```

```{r predict}
rf_pred <- predict(lm4a, newdata = test_a)

rf_rmse <- sqrt(mean((test_a$ph - rf_pred)^2))
rf_mae  <- mean(abs(test_a$ph - rf_pred))

c(RMSE = rf_rmse, MAE = rf_mae)

```

The Random Forest model output a lower lower prediction error (RMSE \~0.09) compared to the linear model (RMSE \~0.13).
This means there may be nonlinear interactions within the process variables.
The improvement wasn't drastic but still meaningful.




